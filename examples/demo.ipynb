{
 "cells": [
  {
   "source": [
    "## Example Notebook Clustering Geo-Data Cubes\n",
    "\n",
    "In this example notebook, we use the `Clustering Geo-Data Cubes` module (`cgc`) to perform co-clustering to a small subset of phenology data (Bloom Conus data, 700x700 pixels, 40 yeasrs). After co-clustering, we use Kmeans method to reduce the number of clusters. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cgc\n",
    "import dask.array as da\n",
    "import json\n",
    "import hashlib\n",
    "\n",
    "from pathlib import Path\n",
    "from osgeo import gdal\n",
    "from dask.distributed import Client, SSHCluster\n",
    "from cgc.kmeans import Kmeans\n",
    "from cgc.coclustering import Coclustering\n",
    "\n",
    "print('CGC version: {}'.format(cgc.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the processing\n",
    "\n",
    "Below are the manual inputs for the processing. They are devided into four parts:\n",
    "\n",
    "1. Project related inputs\n",
    "2. Input data related inputs\n",
    "3. Co-clustering processing related arguments\n",
    "4. K-mean processing related arguments\n",
    "\n",
    "For part `2` and `3` we cache the corresponding steps are cached according to the inputs, i.e., if the inputs do not change, no new computation will be performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Project related\n",
    "project_name = 'Bloom_Europe' # datasetname regionname\n",
    "dir_output = Path('./demo_output') # output data directory\n",
    "\n",
    "# Input data \n",
    "dir_tiff = Path('./demo_data') # path to the tiff files\n",
    "load_pattern = '*.tif' # bash glob pattern of the tiff files to load\n",
    "band_id = 0 # band id to load. If the input tiff only has one band, this value will be ignored\n",
    "\n",
    "# Co-clustering\n",
    "## Changing the following inputs will start a new coclustering processing\n",
    "k = 10  # num clusters in rows\n",
    "l = 5  # num clusters in columns\n",
    "niters = 20\n",
    "errobj, epsilon = 1e-5, 10e-8\n",
    "cache_id = '' # change the cache id to force a new processing\n",
    "## Changing the following inputs won't start a new coclustering processing\n",
    "n_batch = 1 # total number of batches\n",
    "nruns_per_batch = 16 # number of runs per batch, this depends on the available memory\n",
    "mode = 'dask' # choose 'dask' or 'numpy'\n",
    "client = Client() # client for dask, will be ignored if mode == 'numpy'\n",
    "nthreads_numpy = 4 # number of threads for numpy run, will be ignored if mode == 'dask'\n",
    "low_memory = True # low memory processing flag\n",
    "numba_jit = False # numba acceleration flag, will be ignored if mode == 'dask', or if low_memory == False\n",
    "\n",
    "# Kmeans\n",
    "kmean_max_iter = 500 # max number of iterations of kmean\n",
    "k_range = range(2,25) # searching range of value \"k\"\n",
    "variance_threshold = 1.2 # threshold of variance to select \"k\". Check the L-curve plot and readjust this value to select the optimal \"k\""
   ]
  },
  {
   "source": [
    "### Setup environment\n",
    "After mannual configuration, we setup three folders in `dir_output`:\n",
    "\n",
    "1. `cache`: cache data storage. Intermediate results of data loading and co-clustering\n",
    "2. `log`: log files\n",
    "3. `results`: generated plots"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# input cache\n",
    "hash_input = hashlib.sha256()\n",
    "for args in (dir_tiff,load_pattern,band_id):\n",
    "    hash_input.update(str(args).encode('utf-8'))\n",
    "hash_input = hash_input.hexdigest()[:7]\n",
    "print('Input data caching ID: {}'.format(hash_input))\n",
    "\n",
    "# coclustering cache\n",
    "hash_cc = hashlib.sha256()\n",
    "for args in (k,l,errobj,niters,epsilon,cache_id):\n",
    "    hash_cc.update(str(args).encode('utf-8'))\n",
    "hash_cc = hash_cc.hexdigest()[:7]\n",
    "print('CoClustering caching ID: {}'.format(hash_cc))\n",
    "\n",
    "# logging\n",
    "timestamp = '{}'.format(time.strftime('%Y%m%d%H%M%S',time.localtime()))\n",
    "print('Time stamp: {}'.format(timestamp))\n",
    "\n",
    "\n",
    "# Path\n",
    "logdir = (dir_output/'log')\n",
    "resultdir = (dir_output/'results'/('results_{}').format(timestamp))\n",
    "cachedir = (dir_output/'cache')\n",
    "resultdir.mkdir(parents=True, exist_ok=True)\n",
    "logdir.mkdir(parents=True, exist_ok=True)\n",
    "cachedir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s | %(levelname)s : %(message)s',\n",
    "                    level=logging.DEBUG,\n",
    "                    handlers=[logging.FileHandler(logdir/('{}_{}.log'.format(project_name,timestamp)), mode='w'),\n",
    "                              logging.StreamHandler(stream=sys.stdout)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading\n",
    "We loaded data from `dir_tiff`. The loaded tiff files will be specified by `load_pattern`, e.g. `*.tif` means all the tiffs. We assume all tiff files has the same dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load all the geotiffs\n",
    "fname = cachedir/('{}_Z_{}.npy'.format(project_name, hash_input))\n",
    "if not fname.exists():\n",
    "    h_tif = gdal.Open(dir_tiff.as_posix() +'/{}'.format(sorted(dir_tiff.glob(load_pattern))[0].name))\n",
    "    band_flag = True\n",
    "    if h_tif.RasterCount==1:\n",
    "        band_flag = False\n",
    "    Z = np.empty((h_tif.RasterXSize*h_tif.RasterYSize,0))\n",
    "    for f_tiff in sorted(dir_tiff.glob(load_pattern)):\n",
    "        print('loading {}'.format(f_tiff))\n",
    "        h_tif = gdal.Open(f_tiff.as_posix())\n",
    "        if band_flag:\n",
    "            img = h_tif.ReadAsArray(0, 0, h_tif.RasterXSize, h_tif.RasterYSize)[band_id]\n",
    "        else:\n",
    "            img = h_tif.ReadAsArray(0, 0, h_tif.RasterXSize, h_tif.RasterYSize)\n",
    "        img = img.reshape(-1, 1)\n",
    "        Z = np.append(Z, img, axis=1)\n",
    "    Z = Z.astype('float64')\n",
    "    print('Saving loaded tiffs to: {}'.format(fname.as_posix()))\n",
    "    np.save(fname, Z)\n",
    "else:\n",
    "    print('loading cached data: {}'.format(fname.as_posix()))\n",
    "    Z = np.load(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying mask\n",
    "\n",
    "Mask out the pixel if there is an NaN value in its time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Mask out if there is nan in a row\n",
    "mask = np.where(np.isnan(np.sum(Z, axis=1))==False)[0]\n",
    "\n",
    "# Apply mask\n",
    "Znp = Z[mask, :]\n",
    "del Z\n",
    "\n",
    "assert ~np.any(np.sum(np.isnan(Znp), axis=0))"
   ]
  },
  {
   "source": [
    "### Co-clustering"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# Co-clustering\n",
    "fname = cachedir/'./{}_coclustering_{}.json'.format(project_name, hash_cc)\n",
    "if not fname.exists():\n",
    "    for b in range(n_batch):\n",
    "        if mode == 'dask':\n",
    "            Z = da.from_array(Znp) \n",
    "            Z = client.persist(Z)\n",
    "            cc = Coclustering(Z, k, l, errobj, niters, nruns_per_batch, epsilon, output_filename=fname)\n",
    "            cc.run_with_dask(client, low_memory=low_memory)\n",
    "            client.close()\n",
    "        elif mode == 'numpy':\n",
    "            cc = Coclustering(Znp, k, l, errobj, niters, nruns_per_batch, epsilon, output_filename=fname)\n",
    "            cc.run_with_threads(nthreads=nthreads_numpy, low_memory=low_memory, numba_jit=numba_jit)\n",
    "    row_clusters = np.array(cc.results.row_clusters)\n",
    "    col_clusters = np.array(cc.results.col_clusters)\n",
    "else:\n",
    "    with open(fname, 'r') as f:\n",
    "        print('loading cached data: {}'.format(fname.as_posix()))\n",
    "        data = json.load(f)\n",
    "    row_clusters = np.array(data['row_clusters'])\n",
    "    col_clusters = np.array(data['col_clusters'])\n"
   ]
  },
  {
   "source": [
    "### Kmean\n",
    "We search the optimal number of \"k\" within the range `k_range` and an arbitary `var_thres`. The largest \"k\" which gives the variance smaller than `var_thres` will be selected. One may need to check the L-curve plot and manually adjust `var_thres`, to select the optimal \"k\" value."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Kmean\n",
    "km = Kmeans(Z=Znp,\n",
    "            row_clusters=row_clusters,\n",
    "            col_clusters=col_clusters,\n",
    "            n_row_clusters=k,\n",
    "            n_col_clusters=l,\n",
    "            k_range=k_range,\n",
    "            kmean_max_iter=kmean_max_iter,\n",
    "            var_thres=variance_threshold)\n",
    "km.compute()\n",
    "km.plot_elbow_curve((resultdir/(project_name+'_kmean_elbow_plot')).as_posix())\n",
    "km.cl_mean_centroids"
   ]
  },
  {
   "source": [
    "### Visualization: temporal clusters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Export Plots\n",
    "# Temporal cluster\n",
    "plt.plot(range(0,len(col_clusters)),col_clusters)\n",
    "plt.grid(True)\n",
    "plt.ylabel('Temporal Cluster ID')\n",
    "plt.xlabel('Years')\n",
    "plt.savefig((resultdir/(project_name+'_temporal_clusters')).as_posix())\n"
   ]
  },
  {
   "source": [
    "### Visualization: spatial clusters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Spatial cluster\n",
    "h_tif = gdal.Open(dir_tiff.as_posix() +'/{}'.format(sorted(dir_tiff.glob(load_pattern))[0].name))\n",
    "spatial_cl = np.empty(h_tif.RasterXSize*h_tif.RasterYSize)\n",
    "spatial_cl[:] = np.nan\n",
    "spatial_cl[mask] = row_clusters\n",
    "spatial_cl = spatial_cl.reshape(h_tif.RasterYSize, h_tif.RasterXSize)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig = ax.imshow(spatial_cl)\n",
    "plt.title('Spatial Clusters')\n",
    "ax.grid(True)\n",
    "ax.set_xticklabels('')\n",
    "ax.set_yticklabels('')\n",
    "cbar = plt.colorbar(fig, orientation='horizontal')\n",
    "cbar.set_label('Spatial Cluster ID')\n",
    "plt.savefig((resultdir/(project_name+'_spatial_clusters')).as_posix())\n"
   ]
  },
  {
   "source": [
    "### Visualization: group average per temporal cluster"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Export Group average co-clustering\n",
    "CoCavg = np.zeros((k, l)) \n",
    "row_idx = [np.argwhere(row_clusters == i).squeeze() for i in range(k)] \n",
    "col_idx = [np.argwhere(col_clusters == i).squeeze() for i in range(l)]     \n",
    "for ir in range(k): \n",
    "    for ic in range(l): \n",
    "        r, c = np.meshgrid(row_idx[ir], col_idx[ic])\n",
    "        # empty clusters won't be used - the actual num we use below does not matter\n",
    "        CoCavg[ir, ic] = np.nan_to_num(Znp[r, c].mean())\n",
    "        \n",
    "for f in range(l):\n",
    "    # Export png\n",
    "    h_tif = gdal.Open(dir_tiff.as_posix() +'/{}'.format(sorted(dir_tiff.glob(load_pattern))[0].name))\n",
    "    band_1 = np.empty(h_tif.RasterXSize*h_tif.RasterYSize)\n",
    "    band_1[:] =  np.nan\n",
    "    band_1[mask] = CoCavg[row_clusters, f]\n",
    "    band_1 = band_1.reshape(h_tif.RasterYSize, h_tif.RasterXSize)\n",
    "    fig, ax = plt.subplots()\n",
    "    fig = ax.imshow(band_1)\n",
    "    plt.title('Average over temporal cluster ' + str(f))\n",
    "    ax.grid(True)\n",
    "    ax.set_xticklabels('')\n",
    "    ax.set_yticklabels('')\n",
    "    cbar = plt.colorbar(fig, orientation='horizontal')\n",
    "    cbar.set_label('Average value')\n",
    "    fig = ax.imshow(band_1)\n",
    "    plt.savefig((resultdir/(project_name+'_spatial_clusters_temp_clust_' + str(f))).as_posix())"
   ]
  },
  {
   "source": [
    "### Visualization: Kmean \"mean\" centroids per temporal cluster"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Kmean visualization\n",
    "for f in range(l):\n",
    "    # Export png\n",
    "    h_tif = gdal.Open(dir_tiff.as_posix() +'/{}'.format(sorted(dir_tiff.glob(load_pattern))[0].name))\n",
    "    band_1 = np.empty(h_tif.RasterXSize*h_tif.RasterYSize)\n",
    "    band_1[:] =  np.nan\n",
    "    band_1[mask] = km.cl_mean_centroids[row_clusters, f]\n",
    "    band_1 = band_1.reshape(h_tif.RasterYSize, h_tif.RasterXSize)\n",
    "    fig, ax = plt.subplots()\n",
    "    fig = ax.imshow(band_1)\n",
    "    plt.title('Kmean mean centroids: temporal cluster ' + str(f))\n",
    "    ax.grid(True)\n",
    "    ax.set_xticklabels('')\n",
    "    ax.set_yticklabels('')\n",
    "    cbar = plt.colorbar(fig, orientation='horizontal')\n",
    "    cbar.set_label('Kmean mean centroids')\n",
    "    plt.savefig((resultdir/(project_name+'_kmean_spatial_clusters_temp_clust_' + str(f))).as_posix())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('phenology': conda)",
   "language": "python",
   "name": "python38264bitphenologycondaaf35dd4756494be394e1210577c53625"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}